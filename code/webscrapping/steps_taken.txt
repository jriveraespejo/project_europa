steps taken:
0. install build-essential
step needed in the case of using ubuntu
	sudo apt-get install build-essential

1. install python packages (not all will be used)

	1.1 install web browser driver
		- install one of the following according to your web browser:
			. selenium driver (firefox): (josema)
				web: https://github.com/mozilla/geckodriver/releases
				file: geckodriver-v0.29.1-linux64.tar.gz
	
			. selenium driver (Chrome):
				web: https://sites.google.com/a/chromium.org/chromedriver/downloads
				action: select version, select OS
		- extract file
		- make it executable
			chmod +x geckodriver
		- move it to appropriate location
			. sudo mv geckodriver (appropriate location)		
			. (appropriate location) have to be one of the following:  
				. /usr/bin (josema)
				. /usr/local/bin
		- remove file and empty folders (in download folder)
		
	1.2 create virtual environment
		- install/update 'pip' 
			. python3 -m pip install --user -U pip
		- install 'virtualenv' package
			. python3 -m pip install --user -U virtualenv
		- create folder
		- open folder in terminal
		- create virtual environment 
			. virtualenv (environment_name)
		- activate virtual environment
			. source (environment_name)/bin/activate (linux)
			. \(environment_name)\Scripts\activate (Windows)
			
	1.3 install needed packages
		scrappers: 	
			. python3 -m pip install -U bs4 selenium requests lxml lightrdf
		analysis:
			. python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn pystan
		nlp:
			. python3 -m pip install -U spacy nltk itertools
		file control:
			. python3 -m pip install -U os shutil send2trash re glob
		already installed:
			. json
		
	1.4 save requirements.txt
		pip freeze > requirements.txt


2. open IDE
it can be idle, spyder, Jupyter notebook, or other IDE

3. design scrapper 
	- function: 
		. 'bulk_donwload'

4. append '.csv' including dates 
	- function:
		. 'merge_csv'

5. pull from API and join data 
	
	5.1 create an account in https://permid.org/
	
	5.2 match entity's names and instruments (manual process):
		- web: https://permid.org/match
		- there is one file for companies' names (name file):
			. with legal designation (with_*)
			. without legal designation (without_*)
		- function: 
			. 'match_format': to produce the correct csv, per (name file)
	
	5.3 join all matched information
	to join all (name files) produced in the previous step
		- function: 'match_info'
		
	5.4 pull information of entities:
		- function: 'match_data'
			
	5.5 manual identification of missing companies

6. Manual identification of TRBC code for green industries (based on EU taxonomy)

7. merge data of companies and EU taxonomy (based on first 6 digits of TRBC code)
	- functions: 
		. 'merge_taxonomy'
		
		
