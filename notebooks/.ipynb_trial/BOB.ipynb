{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qxue9013_COMP5046_Ass1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8o7UZxWf3Y"
      },
      "source": [
        "If you want to know how data has been saved in pickle file: see this [ipynb file](https://drive.google.com/file/d/1ZQUVBzgH7N2EbiyE3WTPx7JNe2eRTs36/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7C4snIcNl22",
        "outputId": "92b744a5-2d9a-49a2-a90c-ec5de6ddfd52"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "import pickle\n",
        "\n",
        "training_data= pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\"))\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe"
      },
      "source": [
        "\n",
        "\n",
        "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_wKmm6sYnhO"
      },
      "source": [
        "**1** The first task is to remove the @usename strings, punctuations, and website \n",
        "links as they contribute no meaning to the sentences \n",
        "\n",
        "\n",
        "**2** The second task is to reserve the emoticons,and the rest words are changed to the lower-case form. Because it is the sentiment analysis, emoticons does not contribute meaning to the sentence in our analysis. By changing all the words to the lower-case, it will help us to analysis the meaning of the words as Python is case-sensitive \n",
        "\n",
        "**3** The third task is to look at the contraction of the data and to change it. This helps to standardlize the words, which is valuable in this analysis. \n",
        "\n",
        "**4** the fourth task is to remove the stop words which do not add much meaning to the sentence we are going to analysis. \n",
        "\n",
        "\n",
        "**5** the last task is to start the Lemmatisation of the words that can be identified as a single form. This is nessesary in the sentiment analysis as the lemaatisation is based on the meaning of the words \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12"
      },
      "source": [
        "#remove  \"@followed by the username\" in the strings\n",
        "import re\n",
        "def remove_name(x):\n",
        "    x=re.sub(\"@\\S+\",\"\",x)\n",
        "    return x\n",
        "training_data_data=[(x,remove_name(y)) for (x, y) in training_data]\n",
        "testing_data_data=[(x,remove_name(y)) for (x, y) in testing_data]\n",
        "#remove website strings\n",
        "def remove_webname(x):\n",
        "    x=re.sub(\"http\\S+\",\"\",x)\n",
        "    return x \n",
        "training_data_data=[(x,remove_webname(y)) for (x, y) in training_data_data]\n",
        "testing_data_data=[(x,remove_webname(y)) for (x, y) in testing_data_data]\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '','']\n",
        "\n",
        "#remove punctuations \n",
        "def remove_punctuation(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, '')\n",
        "    return x\n",
        "\n",
        "new_training_data_list=[(x,remove_punctuation(y)) for x,y in training_data_data]\n",
        "new_testing_data_list=[(x,remove_punctuation(y)) for x,y in  testing_data_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAz6BGLwZcrT"
      },
      "source": [
        "The emoticons in the list are to be reserved, and the rest words are changed to the lower-case form. Because it is the sentiment analysis, emoticons does not contribute meaning to the sentence in our analysis \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEXg1A2NbN0-"
      },
      "source": [
        "# to reserve the emoticons in the list and change the rest to the lower-case\n",
        "import regex\n",
        "EMOTICONS = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      <3                         # heart\n",
        "    )\"\"\"\n",
        "EMOTICON_RE = regex.compile(EMOTICONS, regex.VERBOSE | regex.I | regex.UNICODE)\n",
        "training=[]\n",
        "for i in new_training_data_list:\n",
        "                    if EMOTICON_RE.search(i[1]):\n",
        "                                      value=i[1]\n",
        "                    else:\n",
        "                       value=i[1].lower()\n",
        "                    training.append((i[0],value))\n",
        "\n",
        "testing=[]\n",
        "for i in new_testing_data_list:\n",
        "                    if EMOTICON_RE.search(i[1]):\n",
        "                                      value=i[1]\n",
        "                    else:\n",
        "                       value=i[1].lower()\n",
        "                    testing.append((i[0],value))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnkaETTGYGxS"
      },
      "source": [
        "# To look at the contraction of the data and to change it. This helps to standardlize the words, which is valuable in this analysis \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBwvGnKOo6Pr",
        "outputId": "e6c6df18-e3c0-4fb8-9c50-de482fb3db70"
      },
      "source": [
        "\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\"cant\": \"cannot\",\"youre\": \"you are\"}\n",
        "def contract(s):\n",
        "  for k in contraction_dict:\n",
        "                     if k in s:\n",
        "                        s = s.replace(k,contraction_dict[k])\n",
        "  return s \n",
        "new_data_training=[(x,contract(y)) for x,y in training ] \n",
        "new_data_testing=[(x,contract(y)) for x,y in testing] \n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = sw.words()\n",
        "a=[(x,word_tokenize(y)) for x, y in new_data_training]\n",
        "b=[(x,word_tokenize(y)) for x, y in new_data_testing]\n",
        "def stop(A):\n",
        "      c=[w for w in A if not w in stop_words]\n",
        "      return c\n",
        "training=[(x,stop(y)) for x, y in a]\n",
        "testing=[(x,stop(y)) for x, y in b]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9nYrJRVX8lr"
      },
      "source": [
        "To start the Lemmatisation of the words that can be identified as a single form. This is nessesary in the sentiment analysis as the lemaatisation is based on the meaning of the words \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey540Sb0Xr7h",
        "outputId": "fe822d69-4c31-4f23-9066-e7a2ea606e95"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "my_training=[]\n",
        "my_testing=[]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for a in training:\n",
        "      my_training.append((a[0],list(map(lemmatizer.lemmatize,a[1]))))\n",
        "for b in testing:\n",
        "      my_testing.append((b[0],list(map(lemmatizer.lemmatize,b[1]))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lq8x9Vz7JHJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-"
      },
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxCMsF6JR7K8"
      },
      "source": [
        "# FastText is used as the model for two reasons. using FastTest is to \"take into account the internal structure of words while learning word representations, which could be very useful for morphologically rich languages, and also for words that occur rarely\"(Wei,2018). Secondly, SkipGram method is dependent on the strong assumption that all the contextual words predicted are independent, while in the RNN model, the previous results are factored as the inputs to the current state for the prediction. Thus, SkipGram is not used here. Instead, i opt for the use of CBOW.\n",
        "\n",
        "\n",
        "(https://www.oreilly.com/library/view/deep-learning-essentials/9781785880360/12fe4a55-a5d0-4712-bd68-ac043b87a87e.xhtml#:~:text=The%20main%20advantage%20of%20FastText,for%20words%20that%20occur%20rarely.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT4cqeMs7OtQ"
      },
      "source": [
        "Dowloading the Ted scrip from the google drive [lab2] \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWXqunJd6fkF",
        "outputId": "6ff5dc1f-7641-42d6-88a9-76297f966d74"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pprint\n",
        "import re\n",
        "from lxml import etree \n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# For data processing\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7NuD3UV8K36"
      },
      "source": [
        "Data preprocessing for the TedScripts \n",
        "1.   the first is to remove \"Sound-effect labels\" using regular expression (regex)[lab 2] \n",
        "2.   the second is to tokenising the sentence as it will lead to the word vector representaio\n",
        "3. punctuations do not add meanings to the sentences.Hence they are deleated \n",
        "4. all capital characters should be converted to the lowercase as python is case senesitive in processing the words \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y140AMW78MM_"
      },
      "source": [
        "# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuation and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg"
      },
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSNPhwlj-oVi"
      },
      "source": [
        "The training data is also preprocessed by \n",
        "\n",
        "\n",
        "1.   removing the @usernames as it does add meaning to the sentecnes\n",
        "2.   removing the website links(http) as they do not add meanings to the sentecnes \n",
        "3.  removing punctuations as they are not useful in this analysis \n",
        "4. reserving the emoticons in the list and change the rest to the lower-case as in the sentiment analysis, we actually care about the sentiment meaning\n",
        "5. changing contractions to its complete forms \n",
        "6. removing the stop words as they do not add much meanings to the sentences \n",
        "7. tokenziing the sentences for the later stage of word processing \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu"
      },
      "source": [
        "#remove  \"@followed by the username\" in the strings as they are not useful for this analysis \n",
        "import re\n",
        "def remove_name(x):\n",
        "    x=re.sub(\"@\\S+\",\"\",x)\n",
        "    return x\n",
        "training_data_data=[(x,remove_name(y)) for (x, y) in training_data]\n",
        "testing_data_data=[(x,remove_name(y)) for (x, y) in testing_data]\n",
        "#remove website strings\n",
        "def remove_webname(x):\n",
        "    x=re.sub(\"http\\S+\",\"\",x)\n",
        "    return x \n",
        "training_data_data=[(x,remove_webname(y)) for (x, y) in training_data]\n",
        "testing_data_data=[(x,remove_webname(y)) for (x, y) in testing_data]\n",
        "\n",
        "\n",
        "#remove punctuations as they are not useful in this analysis \n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '','']\n",
        "\n",
        "def remove_punctuation(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, '')\n",
        "    return x\n",
        "\n",
        "new_training_data_list=[(x,remove_punctuation(y)) for x,y in training_data_data]\n",
        "new_testing_data_list=[(x,remove_punctuation(y)) for x,y in  testing_data_data]\n",
        "\n",
        "\n",
        "\n",
        "# to reserve the emoticons in the list and change the rest to the lower-case\n",
        "import regex\n",
        "EMOTICONS = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      <3                         # heart\n",
        "    )\"\"\"\n",
        "EMOTICON_RE = regex.compile(EMOTICONS, regex.VERBOSE | regex.I | regex.UNICODE)\n",
        "training=[]\n",
        "for i in new_training_data_list:\n",
        "                    if EMOTICON_RE.search(i[1]):\n",
        "                                      value=i[1]\n",
        "                    else:\n",
        "                       value=i[1].lower()\n",
        "                    training.append((i[0],value))\n",
        "\n",
        "testing=[]\n",
        "for i in new_testing_data_list:\n",
        "                    if EMOTICON_RE.search(i[1]):\n",
        "                                      value=i[1]\n",
        "                    else:\n",
        "                       value=i[1].lower()\n",
        "                    testing.append((i[0],value))\n",
        "\n",
        "\n",
        "\n",
        "# to look at the contraction of the data and to change it as this will standardlize the data and helps the analysis \n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\"cant\": \"cannot\",\"youre\": \"you are\"}\n",
        "def contract(s):\n",
        "  for k in contraction_dict:\n",
        "                     if k in s:\n",
        "                        s = s.replace(k,contraction_dict[k])\n",
        "  return s \n",
        "new_data_training=[(x,contract(y)) for x,y in training] \n",
        "new_data_testing=[(x,contract(y)) for x,y in testing] \n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# to remove the stop words as they do not add much meanings to the sentences and thus are not useful\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = sw.words()\n",
        "a=[(x,word_tokenize(y)) for x, y in new_data_training]\n",
        "b=[(x,word_tokenize(y)) for x, y in new_data_testing]\n",
        "def stop(A):\n",
        "      c=[w for w in A if not w in stop_words]\n",
        "      return ' '.join(c)\n",
        "training=[(x,stop(y)) for x, y in a]\n",
        "testing=[(x,stop(y)) for x, y in b]\n",
        "\n",
        "\n",
        "\n",
        "# to start the Lemmatisation of the words that can be identified as a single form. This is nessesary in the sentiment analysis as the lemaatisation is based on the meaning of the words \n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "my_training=[]\n",
        "my_testing=[]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for a in training:\n",
        "      my_training.append((a[0],lemmatizer.lemmatize(a[1])))\n",
        "for b in testing:\n",
        "      my_testing.append((b[0],lemmatizer.lemmatize(b[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYcTSQsZ4tE"
      },
      "source": [
        "The size of embedding is set to 100 as on the one hand, the size can not be too large, which will lead to the curse of dimensionality which increase the computattional time, while on the other hand, it is suggested in the paper of Mikolov(Jeffrey Pennington, 2014] that the increase in dimensionality bring in benefits.Grounded in the emprical evidences from Section 3.1, embedding size of 100 is chosen here. \n",
        "\n",
        "As for the window size, it is set to be 7 following the empirical evidence from Section 3.1. Theoretically,in sentiment analysis, as we are interested in the meaning of the words, the contextual words we aim to analysis should be sufficiently large. \n",
        "\n",
        "To set sg equal to 0, due to the Skip gram's strong assumption on independence of the contetxual words, which is not realistic from the RNN's point of view. \n",
        "\n",
        "Minmum count is set to 1, which is to include  as much as possibel in the analysis. There might be some less frequent words with lexion embedings of value 1 or 2. Those words are also important in our analysis and thus can not be ignored\n",
        "\n",
        "\n",
        "Finally, the dowloaded model of Ted talks is combined with the training and test words after the preprocessing. This is to increase the accuracy score of the model evaluation in the later stage. When the model trained has larger amount of the words, it is expecetd to perform better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU"
      },
      "source": [
        "from gensim.models import  FastText\n",
        "size=100\n",
        "window=7\n",
        "min_count=1\n",
        "workers=8\n",
        "sg=0\n",
        "q=[b for a,b in my_training if len(b)>0 ]\n",
        "g=[b for a,b in my_testing if len(b)>0 ]\n",
        "total=q+g+sentences\n",
        "e=q+g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db395e21-bed1-4188-a22c-b46a381684a6"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "wv_sg_model = FastText(sentences=total, size=size, window=window, min_count=5, workers=workers, sg=sg)\n",
        "word_list=[]\n",
        "for line in q:\n",
        "   for word in line:\n",
        "        if word not in word_list:\n",
        "                     word_list.append(word)\n",
        "word_embedings_training=[]\n",
        "for line in q:\n",
        "        sentence=[]\n",
        "        for word in line:\n",
        "              try: \n",
        "                   sentence.append(wv_sg_model[word])\n",
        "              except:\n",
        "                   pass\n",
        "        word_embedings_training.append(sentence)\n",
        "\n",
        "word_embedings_testing=[]\n",
        "for line in g:\n",
        "        sentence=[]\n",
        "        for word in line:\n",
        "             try:\n",
        "                  sentence.append(wv_sg_model[word])\n",
        "             except:\n",
        "                  pass\n",
        "        word_embedings_testing.append(sentence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Lexicon Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Lexicon-based Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKbLnN-3GlI1"
      },
      "source": [
        "*[Optional] You are required to describe why you would like to use more than one-dimensional embedding.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CUCL1cGlI2"
      },
      "source": [
        "# Please comment your code \n",
        "import pandas as pd \n",
        "negative = pd.read_csv(\"https://gist.githubusercontent.com/x2125001/a84405fe1529e81c6f3d4f8955a2d744/raw/2c8e3f886a9d3133947e08f62442b56bb1f11380/negative.txt\",header=None)\n",
        "negative=[a for a in negative[0]]\n",
        "positive=pd.read_csv(\"https://gist.githubusercontent.com/x2125001/a84405fe1529e81c6f3d4f8955a2d744/raw/2c8e3f886a9d3133947e08f62442b56bb1f11380/positive.txt\",header=None)\n",
        "positive=[a for a in positive[0]]\n",
        "Lexicon_Embedding_dic={}\n",
        "for a in e:\n",
        "        for word in a:\n",
        "            if word in negative:\n",
        "                Lexicon_Embedding_dic[word]=1\n",
        "            elif word in positive:\n",
        "                Lexicon_Embedding_dic[word]=2\n",
        "            else: Lexicon_Embedding_dic[word]=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DiD05Ep3kuv"
      },
      "source": [
        "Lexicon_Embedding_training=[]\n",
        "for line in q:\n",
        "    b=[]\n",
        "    for word in line:\n",
        "       b.append(Lexicon_Embedding_dic[word])\n",
        "    Lexicon_Embedding_training.append(b)\n",
        "\n",
        "Lexicon_Embedding_testing=[]\n",
        "for line in g:\n",
        "    b=[]\n",
        "    for word in line:\n",
        "       b.append(Lexicon_Embedding_dic[word])\n",
        "    Lexicon_Embedding_testing.append(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd"
      },
      "source": [
        "## 2.3. Bi-directional RNN Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Lexicon Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDhGI8IBAWEO"
      },
      "source": [
        "TO concatenate the two embeddings together \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2"
      },
      "source": [
        "# Please comment your code\n",
        "import torch\n",
        "embedings_training=[]\n",
        "for i in range(len(word_embedings_training)):\n",
        "                    sentence=[]\n",
        "                    for j in range(len(word_embedings_training[i])):\n",
        "                          b=list(word_embedings_training[i][j])\n",
        "                          c=Lexicon_Embedding_training[i][j]\n",
        "                          b.append(c)\n",
        "                          sentence.append(b)\n",
        "                    embedings_training.append(sentence)\n",
        "\n",
        "embedings_testing=[]\n",
        "for i in range(len(word_embedings_testing)):\n",
        "                    sentence=[]\n",
        "                    for j in range(len(word_embedings_testing[i])):\n",
        "                          b=list(word_embedings_testing[i][j])\n",
        "                          c=Lexicon_Embedding_testing[i][j]\n",
        "                          b.append(c)\n",
        "                          sentence.append(b)\n",
        "                    embedings_testing.append(sentence) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*\n",
        "\n",
        "The sequence length is set to 25 which is equal to the maxmum length of the sentences in the training and testing data set plus five\n",
        "\n",
        "Number of inputs is set to 101 as the for each word, there is a vector representaion of 101 in length. \n",
        "\n",
        "Number of class is set to 2, which is corresponding to the number of lables in the training data set \"postive\" and \"Negative\"\n",
        "\n",
        "\n",
        "Leaning rate is determined at 0.0005777777777777 between 0.001 and 0.0001. \n",
        "When it is 0.001, it is learning too fast,which might lead to overfiting.And because we have a bigger batch size, we should not have the learning rate that is too small. Thus,0.005777 is chosen here. \n",
        "\n",
        "Number of hidden layer is set be as large as 100 given the large dimension of the input as 101. it is a complex issue, which requires larger number of hidden layers. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6"
      },
      "source": [
        "# Please comment your code\n",
        "# The sequence length is set to 25 which is equal to the maxmum length of the sentences in the training and testing data set plus five.  \n",
        "seq_length = 25\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#\n",
        "def encode_and_add_padding(sentences, seq_length):\n",
        "    sent_encoded = []\n",
        "    for sent in sentences:\n",
        "        c=sent\n",
        "        if len(c) < seq_length:\n",
        "          for i in range(25-len(c)):\n",
        "              c.append([0 for j in range(101)])\n",
        "        sent_encoded.append(c)\n",
        "    return sent_encoded\n",
        "\n",
        "train_pad_encoded = encode_and_add_padding(embedings_training, seq_length )\n",
        "train_pad_encoded=np.asarray(train_pad_encoded)\n",
        "testing_pad_encoded = encode_and_add_padding(embedings_testing, seq_length )\n",
        "testing_pad_encoded=np.asarray(testing_pad_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkacB39tiupB"
      },
      "source": [
        "Label encoding for the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGeKOxSaFbv"
      },
      "source": [
        "training_label=[]\n",
        "for i in my_training:\n",
        "      if len(i[1])<=0:continue\n",
        "      training_label.append(i[0])\n",
        "testing_label=[]\n",
        "for i in my_testing:\n",
        "       if len(i[1])<=0:continue\n",
        "       testing_label.append(i[0])       \n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "unique_labels = np.unique(training_label)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "# Please encode the labels (Do NOT add ne)\n",
        "label_train_encoded = lEnc.fit_transform(training_label)\n",
        "label_test_encoded= lEnc.fit_transform(testing_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDNyxkIlyjg1"
      },
      "source": [
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 22\n",
        "learning_rate = 0.0005777777777777\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eo8-m3-kv8z"
      },
      "source": [
        "import torch\n",
        "# You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Bi_RNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_RNN_Model, self).__init__()\n",
        "        # set the bidirectional to True\n",
        "        self.rnn = nn.RNN(n_input, n_hidden, batch_first =True, bidirectional=True)\n",
        "        self.linear = nn.Linear(2*n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):        \n",
        "        x, h_n = self.rnn(x)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        output = self.linear(hidden_out)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R"
      },
      "source": [
        "### 2.3.3. Train Sequence Model\n",
        "\n",
        "Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVQnUSX1LZ6C",
        "outputId": "59aa34dd-11a0-4c34-860d-5ac157accfb6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, train loss: 42.92588\n",
            "Epoch: 2, train loss: 21.29506\n",
            "Epoch: 3, train loss: 22.48330\n",
            "Epoch: 4, train loss: 21.84997\n",
            "Epoch: 5, train loss: 21.51405\n",
            "Epoch: 6, train loss: 21.29108\n",
            "Epoch: 7, train loss: 21.13336\n",
            "Epoch: 8, train loss: 20.98617\n",
            "Epoch: 9, train loss: 20.86657\n",
            "Epoch: 10, train loss: 20.73856\n",
            "Epoch: 11, train loss: 20.63213\n",
            "Epoch: 12, train loss: 20.51148\n",
            "Epoch: 13, train loss: 20.40532\n",
            "Epoch: 14, train loss: 20.28353\n",
            "Epoch: 15, train loss: 20.15637\n",
            "Epoch: 16, train loss: 20.01383\n",
            "Epoch: 17, train loss: 19.85843\n",
            "Epoch: 18, train loss: 19.71669\n",
            "Epoch: 19, train loss: 19.56661\n",
            "Epoch: 20, train loss: 19.41047\n",
            "Epoch: 21, train loss: 19.25019\n",
            "Epoch: 22, train loss: 19.06828\n",
            "Finished Training\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6302    0.5951    0.6122       988\n",
            "           1     0.6187    0.6529    0.6353       994\n",
            "\n",
            "    accuracy                         0.6241      1982\n",
            "   macro avg     0.6245    0.6240    0.6238      1982\n",
            "weighted avg     0.6244    0.6241    0.6238      1982\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN"
      },
      "source": [
        "# 3 - Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLBzHObsvvM"
      },
      "source": [
        "## 3.1. Word Embedding Evaluation\n",
        "You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSIUsb7qtQEf"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCrcXwcGsuuo",
        "outputId": "a21cad27-6749-4e76-98a2-3998398846d9"
      },
      "source": [
        "size=100\n",
        "window=7\n",
        "min_count=1\n",
        "workers=2\n",
        "sg=0\n",
        "wv_cbow_model = FastText(sentences=total, size=size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
        "wv_cbow_model.wv.save_word2vec_format('combined_cbow_w2v.txt', binary=False)\n",
        "\n",
        "!git clone https://github.com/stanfordnlp/GloVe.git\n",
        "vectors_file=\"/content/combined_cbow_w2v.txt\"\n",
        "with open(vectors_file, 'r') as f:\n",
        "  vectors = {}\n",
        "  for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n",
        "    vals = line.rstrip().split(' ')\n",
        "    vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
        "vocab_words=list(vectors.keys())\n",
        "vocab_size = len(vocab_words)\n",
        "print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "# create word->index and index->word converter\n",
        "vocab = {w: idx for idx, w in enumerate(vocab_words)}\n",
        "ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n",
        "# create the embedding matrix of shape (vocab_size, dim)\n",
        "vector_dim = len(vectors[ivocab[0]])\n",
        "W = np.zeros((vocab_size, vector_dim))\n",
        "for word, v in vectors.items():\n",
        "    if word == '<unk>':\n",
        "        continue\n",
        "    W[vocab[word], :] = v\n",
        "\n",
        "# normalize each word vector to unit length\n",
        "# Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "W_norm = np.zeros(W.shape)\n",
        "d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "W_norm = (W.T / d).T\n",
        "def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions\n",
        "    correct_syn = 0; # count correct syntactic questions\n",
        "    correct_tot = 0 # count correct questions\n",
        "    count_sem = 0; # count all semantic questions\n",
        "    count_syn = 0; # count all syntactic questions\n",
        "    count_tot = 0 # count all questions\n",
        "    full_count = 0 # count all questions, including those with unknown words\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1)\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "\n",
        "        print(\"%s:\" % filenames[i])\n",
        "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
        "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n",
        "correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n",
        "print('Questions seen/total: %.2f%% (%d/%d)' %\n",
        "    (100 * count_tot / float(full_count), count_tot, full_count))\n",
        "print('Semantic accuracy: %.2f%%  (%i/%i)' %\n",
        "    (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
        "print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n",
        "    (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
        "print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GloVe'...\n",
            "remote: Enumerating objects: 595, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 595 (delta 0), reused 1 (delta 0), pack-reused 592\u001b[K\n",
            "Receiving objects: 100% (595/595), 222.33 KiB | 6.01 MiB/s, done.\n",
            "Resolving deltas: 100% (338/338), done.\n",
            "Vocab size:  60037\n",
            "capital-common-countries.txt:\n",
            "ACCURACY TOP1: 0.59% (3/506)\n",
            "capital-world.txt:\n",
            "ACCURACY TOP1: 0.28% (5/1801)\n",
            "currency.txt:\n",
            "ACCURACY TOP1: 0.00% (0/106)\n",
            "city-in-state.txt:\n",
            "ACCURACY TOP1: 0.27% (4/1464)\n",
            "family.txt:\n",
            "ACCURACY TOP1: 11.05% (42/380)\n",
            "gram1-adjective-to-adverb.txt:\n",
            "ACCURACY TOP1: 57.20% (532/930)\n",
            "gram2-opposite.txt:\n",
            "ACCURACY TOP1: 79.50% (477/600)\n",
            "gram3-comparative.txt:\n",
            "ACCURACY TOP1: 64.19% (855/1332)\n",
            "gram4-superlative.txt:\n",
            "ACCURACY TOP1: 80.48% (903/1122)\n",
            "gram5-present-participle.txt:\n",
            "ACCURACY TOP1: 53.88% (569/1056)\n",
            "gram6-nationality-adjective.txt:\n",
            "ACCURACY TOP1: 20.20% (277/1371)\n",
            "gram7-past-tense.txt:\n",
            "ACCURACY TOP1: 10.77% (168/1560)\n",
            "gram8-plural.txt:\n",
            "ACCURACY TOP1: 42.49% (566/1332)\n",
            "gram9-plural-verbs.txt:\n",
            "ACCURACY TOP1: 56.09% (488/870)\n",
            "Questions seen/total: 73.83% (14430/19544)\n",
            "Semantic accuracy: 1.27%  (54/4257)\n",
            "Syntactic accuracy: 47.53%  (4835/10173)\n",
            "Total accuracy: 33.88%  (4889/14430)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pXoqBQEvvoo"
      },
      "source": [
        "Visualize the results by \n",
        "trying dimentionality size in [100,300,500,1000] for Word2vec,\n",
        "window szie in [1,7,9,11], minimum count in [1],and method in [\"cbow\":0, \"skip gram\":1]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmanZey52KoG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "s78_LaR9wDAP",
        "outputId": "5850744e-b6e2-443e-9bf2-ff96979e631c"
      },
      "source": [
        "\n",
        "def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n",
        "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
        "\n",
        "    filenames = [\n",
        "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
        "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
        "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
        "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
        "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
        "        ]\n",
        "\n",
        "    # to avoid memory overflow, could be increased/decreased\n",
        "    # depending on system and vocab size\n",
        "    split_size = 100\n",
        "\n",
        "    correct_sem = 0; # count correct semantic questions\n",
        "    correct_syn = 0; # count correct syntactic questions\n",
        "    correct_tot = 0 # count correct questions\n",
        "    count_sem = 0; # count all semantic questions\n",
        "    count_syn = 0; # count all syntactic questions\n",
        "    count_tot = 0 # count all questions\n",
        "    full_count = 0 # count all questions, including those with unknown words\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
        "            full_data = [line.rstrip().split(' ') for line in f]\n",
        "            full_count += len(full_data)\n",
        "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
        "\n",
        "        if len(data) == 0:\n",
        "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
        "            print(\"Example missing line:\", full_data[0])\n",
        "            continue\n",
        "\n",
        "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
        "        ind1, ind2, ind3, ind4 = indices.T\n",
        "\n",
        "        predictions = np.zeros((len(indices),))\n",
        "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
        "        for j in range(num_iter):\n",
        "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
        "\n",
        "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
        "                +  W[ind3[subset], :])\n",
        "\n",
        "            #cosine similarity if input W has been normalized\n",
        "            dist = np.dot(W, pred_vec.T)\n",
        "\n",
        "\n",
        "            for k in range(len(subset)):\n",
        "                dist[ind1[subset[k]], k] = -np.Inf\n",
        "                dist[ind2[subset[k]], k] = -np.Inf\n",
        "                dist[ind3[subset[k]], k] = -np.Inf\n",
        "\n",
        "            # predicted word index\n",
        "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
        "\n",
        "        \n",
        "        val = (ind4 == predictions) # correct predictions\n",
        "        count_tot = count_tot + len(ind1)\n",
        "        correct_tot = correct_tot + sum(val)\n",
        "        if i < 5:\n",
        "            count_sem = count_sem + len(ind1)\n",
        "            correct_sem = correct_sem + sum(val)\n",
        "        else:\n",
        "            count_syn = count_syn + len(ind1)\n",
        "            correct_syn = correct_syn + sum(val)\n",
        "        \n",
        "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count\n",
        "\n",
        "\n",
        "workers=10\n",
        "dimension=[]\n",
        "window_size=[]\n",
        "min_count_list=[]\n",
        "Syntactic_accuracy=[]\n",
        "Semantic_accuracy=[]\n",
        "Total_accuracy=[]\n",
        "method=[]\n",
        "i=1\n",
        "for size in [100,300,500]:\n",
        "     for window in [1,7,9,]:\n",
        "         for min_count in [1]:\n",
        "            for sg in [0,1]:\n",
        "                wv_cbow_model = FastText(sentences=total, size=size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
        "                wv_cbow_model.wv.save_word2vec_format('combined_cbow_w2v.txt', binary=False)\n",
        "                i+=1\n",
        "                print(i)\n",
        "                !git clone https://github.com/stanfordnlp/GloVe.git\n",
        "                vectors_file=\"/content/combined_cbow_w2v.txt\"\n",
        "                with open(vectors_file, 'r') as f:\n",
        "                     vectors = {}\n",
        "                     for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line \n",
        "                        vals = line.rstrip().split(' ')\n",
        "                        vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
        "                vocab_words=list(vectors.keys())\n",
        "                vocab_size = len(vocab_words)\n",
        "                print(\"Vocab size: \",str(vocab_size))\n",
        "\n",
        "                vocab = {w: idx for idx, w in enumerate(vocab_words)}\n",
        "                ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n",
        "\n",
        "                vector_dim = len(vectors[ivocab[0]])\n",
        "                W = np.zeros((vocab_size, vector_dim))\n",
        "                for word, v in vectors.items():\n",
        "                         if word == '<unk>':\n",
        "                               continue\n",
        "                         W[vocab[word], :] = v\n",
        "                W_norm = np.zeros(W.shape)\n",
        "                d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "                W_norm = (W.T / d).T\n",
        "                correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, vocab, prefix='/content/GloVe/eval/question-data')\n",
        "                Syntactic_accuracy.append(correct_syn / float(count_syn))\n",
        "                Semantic_accuracy.append(correct_sem / float(count_sem))\n",
        "                Total_accuracy.append(correct_tot / float(count_tot))\n",
        "                window_size.append(window)\n",
        "                min_count_list.append(min_count)\n",
        "                method.append(sg)\n",
        "                dimension.append(size)\n",
        "dict_data={}\n",
        "dict_data[\"model\"]=[\"fastText\" for i in range(18)]\n",
        "dict_data[\"minimum_count\"]= min_count_list\n",
        "dict_data[\"dimension\"]=dimension\n",
        "dict_data[\"method\"]=method \n",
        "dict_data[\"window_size\"]=window_size\n",
        "dict_data[\"Total_accuracy\"]=Total_accuracy\n",
        "dict_data[\"Semantic_accuracy\"]= Semantic_accuracy\n",
        "dict_data[\"Syntactic_accuracy\"]= Syntactic_accuracy\n",
        "table=pd.DataFrame(dict_data)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "print(\"method 0=cbow and method 1=skip gram\")\n",
        "\n",
        "table\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "method 0=cbow and method 1=skip gram\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>minimum_count</th>\n",
              "      <th>dimension</th>\n",
              "      <th>method</th>\n",
              "      <th>window_size</th>\n",
              "      <th>Total_accuracy</th>\n",
              "      <th>Semantic_accuracy</th>\n",
              "      <th>Syntactic_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.349965</td>\n",
              "      <td>0.019497</td>\n",
              "      <td>0.488253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.389882</td>\n",
              "      <td>0.022786</td>\n",
              "      <td>0.543497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.340679</td>\n",
              "      <td>0.011510</td>\n",
              "      <td>0.478423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0.408108</td>\n",
              "      <td>0.035706</td>\n",
              "      <td>0.563944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.342065</td>\n",
              "      <td>0.012920</td>\n",
              "      <td>0.479799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0.402980</td>\n",
              "      <td>0.034531</td>\n",
              "      <td>0.557161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>300</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.357588</td>\n",
              "      <td>0.020202</td>\n",
              "      <td>0.498771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>300</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.410534</td>\n",
              "      <td>0.023726</td>\n",
              "      <td>0.572398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>300</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.346500</td>\n",
              "      <td>0.011745</td>\n",
              "      <td>0.486582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>300</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0.419751</td>\n",
              "      <td>0.026310</td>\n",
              "      <td>0.584390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>300</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.340679</td>\n",
              "      <td>0.012685</td>\n",
              "      <td>0.477932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>300</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0.413306</td>\n",
              "      <td>0.023961</td>\n",
              "      <td>0.576231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.357311</td>\n",
              "      <td>0.019262</td>\n",
              "      <td>0.498771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>500</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.403742</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.562961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>500</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.345877</td>\n",
              "      <td>0.011510</td>\n",
              "      <td>0.485796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>500</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0.418642</td>\n",
              "      <td>0.023021</td>\n",
              "      <td>0.584193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>500</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0.344491</td>\n",
              "      <td>0.011745</td>\n",
              "      <td>0.483731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>fastText</td>\n",
              "      <td>1</td>\n",
              "      <td>500</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0.416632</td>\n",
              "      <td>0.023491</td>\n",
              "      <td>0.581146</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       model  minimum_count  dimension  method  window_size  Total_accuracy  \\\n",
              "0   fastText              1        100       0            1        0.349965   \n",
              "1   fastText              1        100       1            1        0.389882   \n",
              "2   fastText              1        100       0            7        0.340679   \n",
              "3   fastText              1        100       1            7        0.408108   \n",
              "4   fastText              1        100       0            9        0.342065   \n",
              "5   fastText              1        100       1            9        0.402980   \n",
              "6   fastText              1        300       0            1        0.357588   \n",
              "7   fastText              1        300       1            1        0.410534   \n",
              "8   fastText              1        300       0            7        0.346500   \n",
              "9   fastText              1        300       1            7        0.419751   \n",
              "10  fastText              1        300       0            9        0.340679   \n",
              "11  fastText              1        300       1            9        0.413306   \n",
              "12  fastText              1        500       0            1        0.357311   \n",
              "13  fastText              1        500       1            1        0.403742   \n",
              "14  fastText              1        500       0            7        0.345877   \n",
              "15  fastText              1        500       1            7        0.418642   \n",
              "16  fastText              1        500       0            9        0.344491   \n",
              "17  fastText              1        500       1            9        0.416632   \n",
              "\n",
              "    Semantic_accuracy  Syntactic_accuracy  \n",
              "0            0.019497            0.488253  \n",
              "1            0.022786            0.543497  \n",
              "2            0.011510            0.478423  \n",
              "3            0.035706            0.563944  \n",
              "4            0.012920            0.479799  \n",
              "5            0.034531            0.557161  \n",
              "6            0.020202            0.498771  \n",
              "7            0.023726            0.572398  \n",
              "8            0.011745            0.486582  \n",
              "9            0.026310            0.584390  \n",
              "10           0.012685            0.477932  \n",
              "11           0.023961            0.576231  \n",
              "12           0.019262            0.498771  \n",
              "13           0.023256            0.562961  \n",
              "14           0.011510            0.485796  \n",
              "15           0.023021            0.584193  \n",
              "16           0.011745            0.483731  \n",
              "17           0.023491            0.581146  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "obH_QXnXn8x9",
        "outputId": "5b040faa-b716-4591-adc5-a6e566f48447"
      },
      "source": [
        " import matplotlib.pyplot as plt\n",
        " constant=table[(table[\"method\"] == 1)]\n",
        " constant=constant[(constant[\"window_size\"] == 7)]\n",
        " plt.plot(constant[\"dimension\"], constant[\"Semantic_accuracy\"], label = \"semantic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " plt.ylabel(\"Semantic_Accuracy\")\n",
        " plt.title('window_size is 7 and method is skip-gram')\n",
        " \n",
        " \n",
        " constant=table[(table[\"method\"] == 1)]\n",
        " constant=constant[(constant[\"window_size\"] == 7)]\n",
        " plt.plot(constant[\"dimension\"], constant[\"Syntactic_accuracy\"], label = \"syntactic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " \n",
        "constant=table[(table[\"method\"] == 1)]\n",
        " constant=constant[(constant[\"window_size\"] == 7)]\n",
        " plt.plot(constant[\"dimension\"], constant[\"Total_accuracy\"], label = \"total accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " plt.ylabel(\"accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb44a20b0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6uL8u-2m0Ve"
      },
      "source": [
        "When we use the model of FastText, keep the minimum count of word as 1, and set up the window size as 7, the increase in the size of dimension does not apperas to dramtically increase the semantic accuracy , total accuracy, and syntactic accuracy. However, we can also see that for semantic accuracy and syntactic accuracy, increase the size of dimension generally increases the accurcy scores \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "hZBtB3UssWTH",
        "outputId": "08c192d8-4060-4466-90f9-4f89e2841c89"
      },
      "source": [
        " import matplotlib.pyplot as plt\n",
        " constant=table[(table[\"method\"] == 0)]\n",
        " constant=constant[(constant[\"window_size\"] == 7)]\n",
        " plt.plot(constant[\"dimension\"], constant[\"Semantic_accuracy\"], label = \"semantic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " plt.ylabel(\"Semantic_Accuracy\")\n",
        " plt.title('window_size is 7 and method is cbow')\n",
        " \n",
        " \n",
        " constant=table[(table[\"method\"] == 0)]\n",
        " constant=constant[(constant[\"window_size\"] == 7)]\n",
        " plt.plot(constant[\"dimension\"], constant[\"Syntactic_accuracy\"], label = \"syntactic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " \n",
        "constant=table[(table[\"method\"] == 0)]\n",
        " constant=constant[(constant[\"window_size\"] == 7)]\n",
        " plt.plot(constant[\"dimension\"], constant[\"Total_accuracy\"], label = \"total accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb447c9d2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKuHhyxos4Al"
      },
      "source": [
        "**At for the methods of CBOW, the synatactic_accuracy, sematic accuracy, and total accuracy do not\n",
        " have significant change as we incerase the size of the dimension, as it is shown above.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "xh36lYIZ17bH",
        "outputId": "08cd6628-2002-4184-8799-cef54724caf9"
      },
      "source": [
        " constant=table[(table[\"method\"] == 0)]\n",
        " constant=constant[(constant[\"dimension\"] == 100)]\n",
        " plt.plot(constant[\"window_size\"], constant[\"Semantic_accuracy\"], label = \"semantic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " plt.ylabel(\"Semantic_Accuracy\")\n",
        " plt.title('dimension is 100  and method is cbow')\n",
        " \n",
        " \n",
        " constant=table[(table[\"method\"] == 0)]\n",
        " constant=constant[(constant[\"dimension\"] == 100)]\n",
        " plt.plot(constant[\"window_size\"], constant[\"Syntactic_accuracy\"], label = \"syntactic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " \n",
        "constant=table[(table[\"method\"] == 0)]\n",
        " constant=constant[(constant[\"dimension\"] == 100)]\n",
        " plt.plot(constant[\"window_size\"], constant[\"Total_accuracy\"], label = \"total accuracy\")\n",
        " plt.xlabel(\"Window_size\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb4478e2150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTi7bMaM3KL3"
      },
      "source": [
        "The incrase in window size also does not benefit the three accuracy scores for CBOW and dimension 100, as it was shown above \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "yRD8YH8_yQJa",
        "outputId": "aacc590d-eafe-4ec5-db7e-afdd0fde1e69"
      },
      "source": [
        " constant=table[(table[\"method\"] == 1)]\n",
        " constant=constant[(constant[\"dimension\"] == 100)]\n",
        " plt.plot(constant[\"window_size\"], constant[\"Semantic_accuracy\"], label = \"semantic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " plt.ylabel(\"Semantic_Accuracy\")\n",
        " plt.title('dimension is 100  and method is skip gram')\n",
        " \n",
        " \n",
        " constant=table[(table[\"method\"] == 1)]\n",
        " constant=constant[(constant[\"dimension\"] == 100)]\n",
        " plt.plot(constant[\"window_size\"], constant[\"Syntactic_accuracy\"], label = \"syntactic accuracy\")\n",
        " plt.xlabel(\"Dimension\")\n",
        " \n",
        "constant=table[(table[\"method\"] == 1)]\n",
        " constant=constant[(constant[\"dimension\"] == 100)]\n",
        " plt.plot(constant[\"window_size\"], constant[\"Total_accuracy\"], label = \"total accuracy\")\n",
        " plt.xlabel(\"Window_size\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb44752b290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fceOoSpf4ytg"
      },
      "source": [
        "The incrase in window size also does not benefit the three accuracy scores for skipgram and dimension 100, as it was shown above \n",
        "\n",
        "In conclusion, it appears that the parameter choice as keeping the minimum count as 1 might have potentially lowered the all accuracy scores. However, it is needed in this model for the benefit of lexicon embeddings as some rare words might have positive or negative sentiments asscoiated with, which are valuable in our model \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr"
      },
      "source": [
        "## 3.2. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCF0bwTtRS0"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f98784-7b87-4efd-b2dd-7205b8291ef4"
      },
      "source": [
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "# More details can be found from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6186    0.5992    0.6087       988\n",
            "           1     0.6137    0.6328    0.6231       994\n",
            "\n",
            "    accuracy                         0.6160      1982\n",
            "   macro avg     0.6161    0.6160    0.6159      1982\n",
            "weighted avg     0.6161    0.6160    0.6159      1982\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo"
      },
      "source": [
        "## 3.3. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.* Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYzrA_s2tTaz"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "c35fab3d-2e2b-4bd3-bcf0-ca5020cfc2e3"
      },
      "source": [
        "f1_score=[]\n",
        "learning=[]\n",
        "optimal_epoch=[]\n",
        "\n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 50\n",
        "learning_rate = 0.0005777777777777 \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 25\n",
        "learning_rate = 0.0005777777777777 \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 125\n",
        "learning_rate = 0.0005777777777777 \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 150\n",
        "learning_rate = 0.0005777777777777 \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "   \n",
        "  \n",
        "  \n",
        " \n",
        " import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 50\n",
        "learning_rate = 0.00001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 25\n",
        "learning_rate = 0.00001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 125\n",
        "learning_rate = 0.00001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 150\n",
        "learning_rate = 0.00001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "  \n",
        "  \n",
        " \n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 50\n",
        "learning_rate = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 25\n",
        "learning_rate = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 125\n",
        "learning_rate = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "import numpy as np\n",
        "n_input = 101 \n",
        "n_class =2 \n",
        "\n",
        "n_hidden = 100\n",
        "batch_size = 256\n",
        "total_epoch = 150\n",
        "learning_rate = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bi_RNN_Model()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_pad_encoded.shape[0],batch_size):\n",
        "        input_batch = train_pad_encoded[ind:min(ind+batch_size, train_pad_encoded.shape[0])]\n",
        "        target_batch = label_train_encoded[ind:min(ind+batch_size,  train_pad_encoded.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(testing_pad_encoded).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "from sklearn.metrics import classification_report\n",
        "a=classification_report(label_test_encoded, predicted.cpu().numpy(),digits=4,output_dict=True)\n",
        "f1_score.append(a[\"weighted avg\"]['f1-score'])\n",
        "learning.append(learning_rate)\n",
        "optimal_epoch.append(total_epoch) \n",
        " \n",
        " \n",
        " dic={}\n",
        "dic[\"optimal_epoch\"]=optimal_epoch\n",
        "dic[\"f1_score\"]=f1_score\n",
        "dic[\"learning_rate\"]=learning \n",
        "Test_hy=pd.DataFrame(dic)\n",
        " \n",
        "\n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "plt.plot( Test_hy[(Test_hy[\"learning_rate\"] == 0.0005777777777777 )][\"optimal_epoch\"],  Test_hy[(Test_hy[\"learning_rate\"] == 0.0005777777777777 )][\"f1_score\"], label = \"learning_rate at{}\".format( 0.0005777777777777 ))\n",
        "plt.plot( Test_hy[(Test_hy[\"learning_rate\"] == 1e-05 )][\"optimal_epoch\"],  Test_hy[(Test_hy[\"learning_rate\"] ==1e-05 )][\"f1_score\"], label =\"learning_rate at{}\".format( 1e-05 ))\n",
        "plt.plot( Test_hy[(Test_hy[\"learning_rate\"] == 0.0001 )][\"optimal_epoch\"],  Test_hy[(Test_hy[\"learning_rate\"] ==0.0001 )][\"f1_score\"], label =\"learning_rate at{}\".format( 0.0001 ))\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.xlabel(\"Number of epoch\")\n",
        "plt.ylabel(\"Average Accuracy Score\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Average Accuracy Score')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc118PbJhbeL"
      },
      "source": [
        "From the graph above, we can see that for different learning rates, when they are smaller than 0.0001, the increase the number of epoch would lead to the increase in the accuracy score. However, it does not hold when the number of epoch reach the certain amount of threshhold. This is because that beyond this treshhold amount, the model begins to be overfiting, which would lower the accuracy score. As for the relatively large learning rate such as 0.0001, the model immediately shows the issue of overfitting at the begining as we increase the number of epoch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVmx4E52dXS"
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}